"""
Integration tests for fetch_data and validate_data Lambda interaction.

These tests focus on the DATA FLOW and CONTRACT between the two lambdas,
NOT on individual function behavior (covered in unit tests).

Integration test scope:
- Data produced by fetch_data handler can be consumed by validate_data handler
- Event output from fetch_data matches input expected by validate_data
- S3 partition paths are consistent between lambdas
- Full pipeline scenarios (stats_only, monthly) work end-to-end
"""

import json
from datetime import datetime
from unittest.mock import MagicMock, patch

import pandas as pd

from src.etl import fetch_data, validate_data


class TestFetchValidateEventContract:
    """Test the event contract between fetch and validate lambdas."""

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_fetch_output_event_is_valid_validate_input(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test that the event output structure from fetch_data handler
        matches the event input structure expected by validate_data handler.
        """
        # Mock minimal valid data
        mock_pergame_df = pd.DataFrame(
            {
                "Player": [f"Player {i}" for i in range(300)],
                "Pos": ["PG"] * 300,
                "Age": [25] * 300,
                "Tm": ["LAL"] * 300,
                "G": [60] * 300,
                "MP": [28.0] * 300,
                "PTS": [15.0] * 300,
                "TRB": [5.0] * 300,
                "AST": [4.0] * 300,
            }
        )
        mock_advanced_df = mock_pergame_df.copy()
        mock_advanced_df["PER"] = [15.0] * 300
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        captured_data = {}
        mock_save_s3.side_effect = lambda data, key: captured_data.update({key: data}) or True
        mock_load_s3.side_effect = lambda key: captured_data.get(key)

        # Execute fetch
        fetch_result = fetch_data.handler({"fetch_type": "stats_only"}, MagicMock())

        # Verify fetch returns required fields for validate
        assert "data_location" in fetch_result
        assert "bucket" in fetch_result["data_location"]
        assert "partition" in fetch_result["data_location"]

        # Execute validate using fetch's output
        validate_event = {
            "data_location": fetch_result["data_location"],
            "fetch_type": "stats_only",
        }
        validate_result = validate_data.handler(validate_event, MagicMock())

        # Verify validate accepts and processes the event
        assert validate_result["statusCode"] == 200
        assert "validation_passed" in validate_result

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_partition_path_used_consistently(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test that both lambdas use the same partition path for S3 operations.
        Validates that S3 keys generated by fetch match keys used by validate.
        """
        mock_pergame_df = pd.DataFrame({"Player": [f"P{i}" for i in range(300)]})
        mock_advanced_df = pd.DataFrame({"Player": [f"P{i}" for i in range(300)]})
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        saved_keys = []
        loaded_keys = []

        def capture_save(data, key):
            saved_keys.append(key)
            return True

        def capture_load(key):
            loaded_keys.append(key)
            return {
                "season": "2025-26",
                "fetch_timestamp": datetime.utcnow().isoformat(),
                "source": "basketball_reference",
                "per_game_stats": [{"Player": "P"}],
                "advanced_stats": [{"Player": "P"}],
                "per_game_columns": ["Player", "Pos", "Age", "Tm", "G", "MP", "PTS", "TRB", "AST"],
                "advanced_columns": ["Player", "Pos", "Age", "Tm", "G", "MP", "PER"],
            }

        mock_save_s3.side_effect = capture_save
        mock_load_s3.side_effect = capture_load

        # Execute fetch
        fetch_result = fetch_data.handler({"fetch_type": "stats_only"}, MagicMock())
        partition = fetch_result["data_location"]["partition"]

        # Verify fetch saved to partition path
        assert len(saved_keys) > 0
        assert all(partition in key for key in saved_keys)

        # Execute validate
        validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        # Verify validate loaded from same partition path
        assert len(loaded_keys) > 0
        assert all(partition in key for key in loaded_keys)


class TestEndToEndDataFlow:
    """Test complete data flow from fetch through validate."""

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_stats_only_pipeline_end_to_end(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test complete stats_only pipeline: fetch saves data -> validate loads and validates.
        This tests the actual data handoff between lambdas.
        """
        # Mock Basketball-Reference data
        mock_pergame_df = pd.DataFrame(
            {
                "Player": ["LeBron James"] + [f"Player {i}" for i in range(299)],
                "Pos": ["SF"] + ["PG"] * 299,
                "Age": [40] + [25] * 299,
                "Tm": ["LAL"] * 300,
                "G": [50] + [60] * 299,
                "MP": [35.0] + [28.0] * 299,
                "PTS": [25.0] + [15.0] * 299,
                "TRB": [7.5] + [5.0] * 299,
                "AST": [8.2] + [4.0] * 299,
            }
        )
        mock_advanced_df = mock_pergame_df.copy()
        mock_advanced_df["PER"] = [24.5] + [15.0] * 299
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        # Simulate actual S3 storage - fetch saves, validate loads
        s3_storage = {}

        def save_to_s3_impl(data, key):
            s3_storage[key] = data
            return True

        def load_from_s3_impl(key):
            return s3_storage.get(key)

        mock_save_s3.side_effect = save_to_s3_impl
        mock_load_s3.side_effect = load_from_s3_impl

        # Step 1: Fetch data
        fetch_result = fetch_data.handler(
            {"fetch_type": "stats_only", "season": "2025-26"}, MagicMock()
        )

        assert fetch_result["statusCode"] == 200
        fetch_body = json.loads(fetch_result["body"])
        assert "player_stats" in fetch_body["fetched"]

        # Step 2: Validate the fetched data
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 200
        assert validate_result["validation_passed"] is True
        validate_body = json.loads(validate_result["body"])
        assert validate_body["valid"] is True

    @patch("src.etl.fetch_data.fetch_espn_salaries")
    @patch("src.etl.fetch_data.teams.get_teams")
    @patch("src.etl.fetch_data.players.get_active_players")
    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_monthly_pipeline_all_data_types(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
        mock_get_players,
        mock_get_teams,
        mock_fetch_salaries,
    ):
        """
        Test monthly pipeline with all 4 data types: players, stats, teams, salaries.
        Validates that all data types flow correctly from fetch to validate.
        """
        # Mock all data sources
        mock_get_players.return_value = [
            {
                "id": i,
                "full_name": f"Player {i}",
                "first_name": "P",
                "last_name": f"{i}",
                "is_active": True,
            }
            for i in range(400)
        ]

        mock_get_teams.return_value = [
            {
                "id": 1610612740 + i,
                "full_name": f"Team {i}",
                "abbreviation": f"T{i:02d}",
                "nickname": f"T{i}",
                "city": "City",
                "state": "State",
                "year_founded": 1947,
            }
            for i in range(30)
        ]

        mock_pergame_df = pd.DataFrame(
            {
                "Player": [f"Player {i}" for i in range(300)],
                "Pos": ["PG"] * 300,
                "Age": [25] * 300,
                "Tm": ["LAL"] * 300,
                "G": [60] * 300,
                "MP": [28.0] * 300,
                "PTS": [15.0] * 300,
                "TRB": [5.0] * 300,
                "AST": [4.0] * 300,
            }
        )
        mock_advanced_df = mock_pergame_df.copy()
        mock_advanced_df["PER"] = [15.0] * 300
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        mock_fetch_salaries.return_value = [
            {
                "player_name": f"Player {i}",
                "annual_salary": 1000000 + i,
                "season": "2025-26",
                "source": "espn",
            }
            for i in range(400)
        ]

        # Simulate S3 storage
        s3_storage = {}
        mock_save_s3.side_effect = lambda data, key: s3_storage.update({key: data}) or True
        mock_load_s3.side_effect = lambda key: s3_storage.get(key)

        # Execute fetch (monthly mode)
        fetch_result = fetch_data.handler(
            {"fetch_type": "monthly", "season": "2025-26"}, MagicMock()
        )

        assert fetch_result["statusCode"] == 200
        fetch_body = json.loads(fetch_result["body"])
        # Verify all 4 data types were fetched
        assert "active_players" in fetch_body["fetched"]
        assert "player_stats" in fetch_body["fetched"]
        assert "teams" in fetch_body["fetched"]
        assert "salaries" in fetch_body["fetched"]

        # Execute validate (should validate all 4 data types)
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "monthly"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 200
        assert validate_result["validation_passed"] is True


class TestCrossLambdaErrorDetection:
    """Test that validate catches data quality issues from fetch."""

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_validate_rejects_incomplete_fetch_data(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test that validate rejects data from fetch when required columns are missing.
        This tests cross-lambda data quality enforcement.
        """
        # Fetch produces incomplete data (missing required columns)
        mock_pergame_df = pd.DataFrame({"Player": ["Test"], "PTS": [20.0]})
        mock_advanced_df = pd.DataFrame({"Player": ["Test"], "PER": [15.0]})
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        s3_storage = {}
        mock_save_s3.side_effect = lambda data, key: s3_storage.update({key: data}) or True
        mock_load_s3.side_effect = lambda key: s3_storage.get(key)

        # Fetch completes (doesn't validate structure)
        fetch_result = fetch_data.handler({"fetch_type": "stats_only"}, MagicMock())
        assert fetch_result["statusCode"] == 200

        # Validate should catch the incomplete data
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 422
        assert validate_result["validation_passed"] is False

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_validate_handles_fetch_save_failure(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test that validate properly reports when fetch fails to save data to S3.
        This tests error propagation across lambda boundaries.
        """
        mock_read_html.side_effect = [[pd.DataFrame()], [pd.DataFrame()]]
        mock_save_s3.return_value = False  # S3 save fails
        mock_load_s3.return_value = None  # No data in S3

        # Fetch fails to save
        fetch_result = fetch_data.handler({"fetch_type": "stats_only"}, MagicMock())
        fetch_body = json.loads(fetch_result["body"])
        assert len(fetch_body["errors"]) > 0

        # Validate should report missing data
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 422
        assert validate_result["validation_passed"] is False
