"""
Integration tests for fetch_data and validate_data Lambda interaction.

These tests focus on the DATA FLOW and CONTRACT between the two lambdas,
NOT on individual function behavior (covered in unit tests).

Integration test scope:
- Data produced by fetch_data handler can be consumed by validate_data handler
- Event output from fetch_data matches input expected by validate_data
- S3 partition paths are consistent between lambdas
- Full pipeline scenarios (stats_only, monthly) work end-to-end
"""

import json
from unittest.mock import MagicMock, patch

import pandas as pd

from src.etl import fetch_data, validate_data
from tests.integration.conftest import (
    create_basketball_reference_advanced_stats,
    create_basketball_reference_player_stats,
)


class TestFetchValidateEventContract:
    """Test the event contract between fetch and validate lambdas."""

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_fetch_output_event_is_valid_validate_input(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test that the event output structure from fetch_data handler
        matches the event input structure expected by validate_data handler.
        """
        # Mock complete Basketball Reference formatted data
        per_game_stats = [
            create_basketball_reference_player_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 5.0, 4.0
            )
            for i in range(350)
        ]
        mock_pergame_df = pd.DataFrame(per_game_stats)

        # Advanced stats with complete columns
        advanced_stats = [
            create_basketball_reference_advanced_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 2.0
            )
            for i in range(350)
        ]
        mock_advanced_df = pd.DataFrame(advanced_stats)
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        captured_data = {}
        mock_save_s3.side_effect = lambda data, key: captured_data.update({key: data}) or True
        mock_load_s3.side_effect = lambda key: captured_data.get(key)

        # Execute fetch
        fetch_result = fetch_data.handler({"fetch_type": "stats_only"}, MagicMock())

        # Verify fetch returns required fields for validate
        assert "data_location" in fetch_result
        assert "bucket" in fetch_result["data_location"]
        assert "partition" in fetch_result["data_location"]

        # Execute validate using fetch's output
        validate_event = {
            "data_location": fetch_result["data_location"],
            "fetch_type": "stats_only",
        }
        validate_result = validate_data.handler(validate_event, MagicMock())

        # Verify validate accepts and processes the event
        assert validate_result["statusCode"] == 200
        assert "validation_passed" in validate_result

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_partition_path_used_consistently(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test that both lambdas use the same partition path for S3 operations.
        Validates that S3 keys generated by fetch match keys used by validate.
        """
        # Mock complete Basketball Reference formatted data
        per_game_stats = [
            create_basketball_reference_player_stats(f"P{i}", "PG", 25, "LAL", 60, 15.0, 5.0, 4.0)
            for i in range(350)
        ]
        mock_pergame_df = pd.DataFrame(per_game_stats)
        advanced_stats = [
            create_basketball_reference_advanced_stats(f"P{i}", "PG", 25, "LAL", 60, 15.0, 2.0)
            for i in range(350)
        ]
        mock_advanced_df = pd.DataFrame(advanced_stats)
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        saved_keys = []
        loaded_keys = []
        captured_data = {}

        def capture_save(data, key):
            saved_keys.append(key)
            captured_data[key] = data
            return True

        def capture_load(key):
            loaded_keys.append(key)
            return captured_data.get(key)

        mock_save_s3.side_effect = capture_save
        mock_load_s3.side_effect = capture_load

        # Execute fetch
        fetch_result = fetch_data.handler({"fetch_type": "stats_only"}, MagicMock())
        partition = fetch_result["data_location"]["partition"]

        # Verify fetch saved to partition path
        assert len(saved_keys) > 0
        assert all(partition in key for key in saved_keys)

        # Execute validate
        validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        # Verify validate loaded from same partition path
        assert len(loaded_keys) > 0
        assert all(partition in key for key in loaded_keys)


class TestEndToEndDataFlow:
    """Test complete data flow from fetch through validate."""

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_stats_only_pipeline_end_to_end(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test complete stats_only pipeline: fetch saves data -> validate loads and validates.
        This tests the actual data handoff between lambdas.
        """
        # Mock complete Basketball-Reference data
        per_game_stats = [
            create_basketball_reference_player_stats(
                "LeBron James", "SF", 40, "LAL", 50, 25.0, 7.5, 8.2
            )
        ] + [
            create_basketball_reference_player_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 5.0, 4.0
            )
            for i in range(349)
        ]
        mock_pergame_df = pd.DataFrame(per_game_stats)
        advanced_stats = [
            create_basketball_reference_advanced_stats(
                "LeBron James", "SF", 40, "LAL", 50, 24.5, 4.0
            )
        ] + [
            create_basketball_reference_advanced_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 2.0
            )
            for i in range(349)
        ]
        mock_advanced_df = pd.DataFrame(advanced_stats)
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        # Simulate actual S3 storage - fetch saves, validate loads
        s3_storage = {}

        def save_to_s3_impl(data, key):
            s3_storage[key] = data
            return True

        def load_from_s3_impl(key):
            return s3_storage.get(key)

        mock_save_s3.side_effect = save_to_s3_impl
        mock_load_s3.side_effect = load_from_s3_impl

        # Step 1: Fetch data
        fetch_result = fetch_data.handler(
            {"fetch_type": "stats_only", "season": "2025-26"}, MagicMock()
        )

        assert fetch_result["statusCode"] == 200
        fetch_body = json.loads(fetch_result["body"])
        assert "player_stats" in fetch_body["fetched"]

        # Step 2: Validate the fetched data
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 200
        assert validate_result["validation_passed"] is True
        validate_body = json.loads(validate_result["body"])
        assert validate_body["valid"] is True

    @patch("src.etl.fetch_data.fetch_espn_salaries")
    @patch("src.etl.fetch_data.teams.get_teams")
    @patch("src.etl.fetch_data.players.get_active_players")
    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_monthly_pipeline_all_data_types(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
        mock_get_players,
        mock_get_teams,
        mock_fetch_salaries,
    ):
        """
        Test monthly pipeline with all 4 data types: players, stats, teams, salaries.
        Validates that all data types flow correctly from fetch to validate.
        """
        # Mock all data sources
        mock_get_players.return_value = [
            {
                "id": i,
                "full_name": f"Player {i}",
                "first_name": "P",
                "last_name": f"{i}",
                "is_active": True,
            }
            for i in range(400)
        ]

        mock_get_teams.return_value = [
            {
                "id": 1610612740 + i,
                "full_name": f"Team {i}",
                "abbreviation": f"T{i:02d}",
                "nickname": f"T{i}",
                "city": "City",
                "state": "State",
                "year_founded": 1947,
            }
            for i in range(30)
        ]

        # Mock complete Basketball-Reference data
        per_game_stats = [
            create_basketball_reference_player_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 5.0, 4.0
            )
            for i in range(350)
        ]
        mock_pergame_df = pd.DataFrame(per_game_stats)
        advanced_stats = [
            create_basketball_reference_advanced_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 2.0
            )
            for i in range(350)
        ]
        mock_advanced_df = pd.DataFrame(advanced_stats)
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        mock_fetch_salaries.return_value = [
            {
                "player_name": f"Player {i}",
                "annual_salary": 1000000 + i,
                "season": "2025-26",
                "source": "espn",
            }
            for i in range(400)
        ]

        # Simulate S3 storage
        s3_storage = {}
        mock_save_s3.side_effect = lambda data, key: s3_storage.update({key: data}) or True
        mock_load_s3.side_effect = lambda key: s3_storage.get(key)

        # Execute fetch (monthly mode)
        fetch_result = fetch_data.handler(
            {"fetch_type": "monthly", "season": "2025-26"}, MagicMock()
        )

        assert fetch_result["statusCode"] == 200
        fetch_body = json.loads(fetch_result["body"])
        # Verify all 4 data types were fetched
        assert "active_players" in fetch_body["fetched"]
        assert "player_stats" in fetch_body["fetched"]
        assert "teams" in fetch_body["fetched"]
        assert "salaries" in fetch_body["fetched"]

        # Execute validate (should validate all 4 data types)
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "monthly"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 200
        assert validate_result["validation_passed"] is True


class TestCrossLambdaErrorDetection:
    """Test that validate catches data quality issues from fetch."""

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_validate_rejects_incomplete_fetch_data(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test that validate rejects data from fetch when required columns are missing.
        This tests cross-lambda data quality enforcement.
        """
        # Fetch produces incomplete data (missing required columns)
        mock_pergame_df = pd.DataFrame({"Player": ["Test"], "PTS": [20.0]})
        mock_advanced_df = pd.DataFrame({"Player": ["Test"], "PER": [15.0]})
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        s3_storage = {}
        mock_save_s3.side_effect = lambda data, key: s3_storage.update({key: data}) or True
        mock_load_s3.side_effect = lambda key: s3_storage.get(key)

        # Fetch completes (doesn't validate structure)
        fetch_result = fetch_data.handler({"fetch_type": "stats_only"}, MagicMock())
        assert fetch_result["statusCode"] == 200

        # Validate should catch the incomplete data
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 422
        assert validate_result["validation_passed"] is False

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    def test_validate_handles_fetch_save_failure(
        self,
        mock_save_report,
        mock_load_s3,
        mock_save_s3,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test that validate properly reports when fetch fails to save data to S3.
        This tests error propagation across lambda boundaries.
        """
        mock_read_html.side_effect = [[pd.DataFrame()], [pd.DataFrame()]]
        mock_save_s3.return_value = False  # S3 save fails
        mock_load_s3.return_value = None  # No data in S3

        # Fetch fails to save
        fetch_result = fetch_data.handler({"fetch_type": "stats_only"}, MagicMock())
        fetch_body = json.loads(fetch_result["body"])
        assert len(fetch_body["errors"]) > 0

        # Validate should report missing data
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 422
        assert validate_result["validation_passed"] is False


class TestCompletePipelineFlow:
    """Test complete 3-stage pipeline: fetch → validate → transform."""

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.transform_data.load_from_s3")
    @patch("src.etl.transform_data.save_to_s3")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    @patch("src.etl.transform_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.transform_data.ENVIRONMENT", "test")
    def test_stats_only_complete_pipeline(
        self,
        mock_transform_save,
        mock_transform_load,
        mock_validate_save_report,
        mock_validate_load,
        mock_fetch_save,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test complete stats_only pipeline end-to-end:
        Fetch stats → Validate stats → Transform stats (without salary/player data).
        Verify enriched_player_stats created successfully.
        """
        # Mock complete Basketball-Reference data for fetch
        per_game_stats = [
            create_basketball_reference_player_stats(
                "LeBron James", "SF", 40, "LAL", 50, 25.0, 7.5, 8.0
            ),
            create_basketball_reference_player_stats(
                "Anthony Davis", "C", 31, "LAL", 55, 27.0, 12.0, 3.5
            ),
        ] + [
            create_basketball_reference_player_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 5.0, 4.0
            )
            for i in range(348)
        ]
        mock_pergame_df = pd.DataFrame(per_game_stats)
        advanced_stats = [
            create_basketball_reference_advanced_stats(
                "LeBron James", "SF", 40, "LAL", 50, 24.5, 4.0
            ),
            create_basketball_reference_advanced_stats(
                "Anthony Davis", "C", 31, "LAL", 55, 26.0, 5.0
            ),
        ] + [
            create_basketball_reference_advanced_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 2.0
            )
            for i in range(348)
        ]
        mock_advanced_df = pd.DataFrame(advanced_stats)
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        # Simulate S3 storage
        s3_storage = {}

        def save_impl(data, key):
            s3_storage[key] = data
            return True

        def load_impl(key):
            return s3_storage.get(key)

        mock_fetch_save.side_effect = save_impl
        mock_validate_load.side_effect = load_impl
        mock_transform_load.side_effect = load_impl
        mock_transform_save.side_effect = save_impl

        # Stage 1: Fetch data
        from src.etl import fetch_data

        fetch_result = fetch_data.handler(
            {"fetch_type": "stats_only", "season": "2025-26"}, MagicMock()
        )

        assert fetch_result["statusCode"] == 200
        fetch_body = json.loads(fetch_result["body"])
        assert "player_stats" in fetch_body["fetched"]

        # Stage 2: Validate data
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 200
        assert validate_result["validation_passed"] is True

        # Stage 3: Transform data
        from src.etl import transform_data

        transform_result = transform_data.handler(
            {
                "validation_passed": validate_result["validation_passed"],
                "data_location": validate_result["data_location"],
            },
            MagicMock(),
        )

        assert transform_result["statusCode"] == 200
        assert transform_result["transformation_successful"] is True

        # Verify enriched stats were created
        transform_body = json.loads(transform_result["body"])
        assert "enriched_player_stats" in transform_body["transformed"]
        assert transform_body["statistics"]["total_player_stats"] == 350

        # Verify enriched data structure in S3 storage
        stats_key = [k for k in s3_storage.keys() if "enriched_player_stats" in k][0]
        enriched_stats = s3_storage[stats_key]
        assert len(enriched_stats["player_stats"]) == 350

        # Verify LeBron's stats are correctly merged
        lebron = next(
            p for p in enriched_stats["player_stats"] if p["player_name"] == "LeBron James"
        )
        assert lebron["points"] == 25.0
        assert lebron["per"] == 24.5

    @patch("src.etl.fetch_data.fetch_espn_salaries")
    @patch("src.etl.fetch_data.teams.get_teams")
    @patch("src.etl.fetch_data.players.get_active_players")
    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.transform_data.load_from_s3")
    @patch("src.etl.transform_data.save_to_s3")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    @patch("src.etl.transform_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.transform_data.ENVIRONMENT", "test")
    def test_monthly_complete_pipeline(
        self,
        mock_transform_save,
        mock_transform_load,
        mock_validate_save_report,
        mock_validate_load,
        mock_fetch_save,
        mock_sleep,
        mock_read_html,
        mock_get_players,
        mock_get_teams,
        mock_fetch_salaries,
    ):
        """
        Test complete monthly pipeline end-to-end:
        Fetch all 4 data types → Validate all → Transform all.
        Verify all 3 enriched outputs: salaries, stats, teams.
        """
        import pandas as pd

        # Mock all data sources for fetch
        mock_get_players.return_value = [
            {"id": 2544, "full_name": "LeBron James"},
            {"id": 203076, "full_name": "Anthony Davis"},
        ]

        mock_get_teams.return_value = [
            {
                "id": 1610612747,
                "full_name": "Los Angeles Lakers",
                "abbreviation": "LAL",
                "nickname": "Lakers",
                "city": "Los Angeles",
                "state": "California",
                "year_founded": 1947,
            }
        ]

        per_game_stats = [
            create_basketball_reference_player_stats(
                "LeBron James", "SF", 40, "LAL", 50, 25.0, 7.5, 8.0
            ),
            create_basketball_reference_player_stats(
                "Anthony Davis", "C", 31, "LAL", 55, 27.0, 12.0, 3.5
            ),
        ]
        mock_pergame_df = pd.DataFrame(per_game_stats)
        advanced_stats = [
            create_basketball_reference_advanced_stats(
                "LeBron James", "SF", 40, "LAL", 50, 24.5, 4.0
            ),
            create_basketball_reference_advanced_stats(
                "Anthony Davis", "C", 31, "LAL", 55, 26.0, 5.0
            ),
        ]
        mock_advanced_df = pd.DataFrame(advanced_stats)
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        mock_fetch_salaries.return_value = [
            {
                "player_name": "LeBron James",
                "annual_salary": 48000000,
                "season": "2025-26",
                "source": "espn",
            },
            {
                "player_name": "Anthony Davis",
                "annual_salary": 55000000,
                "season": "2025-26",
                "source": "espn",
            },
        ]

        # Simulate S3 storage
        s3_storage = {}

        def save_impl(data, key):
            s3_storage[key] = data
            return True

        def load_impl(key):
            return s3_storage.get(key)

        mock_fetch_save.side_effect = save_impl
        mock_validate_load.side_effect = load_impl
        mock_transform_load.side_effect = load_impl
        mock_transform_save.side_effect = save_impl

        # Stage 1: Fetch all data
        from src.etl import fetch_data

        fetch_result = fetch_data.handler(
            {"fetch_type": "monthly", "season": "2025-26"}, MagicMock()
        )

        assert fetch_result["statusCode"] == 200
        fetch_body = json.loads(fetch_result["body"])
        assert "active_players" in fetch_body["fetched"]
        assert "player_stats" in fetch_body["fetched"]
        assert "teams" in fetch_body["fetched"]
        assert "salaries" in fetch_body["fetched"]

        # Stage 2: Validate all data
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "monthly"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 200
        assert validate_result["validation_passed"] is True

        # Stage 3: Transform all data
        from src.etl import transform_data

        transform_result = transform_data.handler(
            {
                "validation_passed": validate_result["validation_passed"],
                "data_location": validate_result["data_location"],
            },
            MagicMock(),
        )

        assert transform_result["statusCode"] == 200
        assert transform_result["transformation_successful"] is True

        # Verify all 3 enriched outputs were created
        transform_body = json.loads(transform_result["body"])
        assert "enriched_salaries" in transform_body["transformed"]
        assert "enriched_player_stats" in transform_body["transformed"]
        assert "enriched_teams" in transform_body["transformed"]

        # Verify enriched_salaries has player_id field
        salary_key = [k for k in s3_storage.keys() if "enriched_salaries" in k][0]
        enriched_salaries = s3_storage[salary_key]
        assert all(s.get("player_id") is not None for s in enriched_salaries["salaries"])
        assert enriched_salaries["statistics"]["match_rate"] == 100.0

        # Verify enriched_stats merges per-game + advanced
        stats_key = [k for k in s3_storage.keys() if "enriched_player_stats" in k][0]
        enriched_stats = s3_storage[stats_key]
        assert len(enriched_stats["player_stats"]) == 2
        lebron = enriched_stats["player_stats"][0]
        assert lebron["points"] == 25.0
        assert lebron["per"] == 24.5

        # Verify enriched_teams has aggregations
        teams_key = [k for k in s3_storage.keys() if "enriched_teams" in k][0]
        enriched_teams = s3_storage[teams_key]
        assert len(enriched_teams["teams"]) == 1
        lakers = enriched_teams["teams"][0]
        # Verify team has aggregation fields (exact values depend on name matching)
        assert "total_payroll" in lakers
        assert "roster_count" in lakers
        assert "roster_with_salary" in lakers

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.transform_data.load_from_s3")
    @patch("src.etl.transform_data.save_to_s3")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    @patch("src.etl.transform_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.transform_data.ENVIRONMENT", "test")
    def test_bad_fetch_data_rejected_by_validate_blocks_transform(
        self,
        mock_transform_save,
        mock_transform_load,
        mock_validate_save_report,
        mock_validate_load,
        mock_fetch_save,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test error propagation through entire pipeline:
        Fetch incomplete data → Validate fails → Transform skips.
        """
        import pandas as pd

        # Mock incomplete data (missing required columns)
        mock_pergame_df = pd.DataFrame({"Player": ["Test Player"], "PTS": [20.0]})
        mock_advanced_df = pd.DataFrame({"Player": ["Test Player"], "PER": [15.0]})
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        # Simulate S3 storage
        s3_storage = {}

        def save_impl(data, key):
            s3_storage[key] = data
            return True

        def load_impl(key):
            return s3_storage.get(key)

        mock_fetch_save.side_effect = save_impl
        mock_validate_load.side_effect = load_impl
        mock_transform_load.side_effect = load_impl
        mock_transform_save.side_effect = save_impl

        # Stage 1: Fetch incomplete data
        from src.etl import fetch_data

        fetch_result = fetch_data.handler(
            {"fetch_type": "stats_only", "season": "2025-26"}, MagicMock()
        )

        assert fetch_result["statusCode"] == 200  # Fetch doesn't validate structure

        # Stage 2: Validate (should fail)
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["statusCode"] == 422
        assert validate_result["validation_passed"] is False

        # Stage 3: Transform should skip processing
        from src.etl import transform_data

        transform_result = transform_data.handler(
            {
                "validation_passed": validate_result["validation_passed"],
                "data_location": validate_result["data_location"],
            },
            MagicMock(),
        )

        assert transform_result["statusCode"] == 400
        body = json.loads(transform_result["body"])
        assert "validation failed" in body["error"].lower()

    @patch("src.etl.fetch_data.pd.read_html")
    @patch("src.etl.fetch_data.time.sleep")
    @patch("src.etl.fetch_data.save_to_s3")
    @patch("src.etl.validate_data.load_from_s3")
    @patch("src.etl.validate_data.save_validation_report")
    @patch("src.etl.transform_data.load_from_s3")
    @patch("src.etl.transform_data.save_to_s3")
    @patch("src.etl.fetch_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.fetch_data.ENVIRONMENT", "test")
    @patch("src.etl.validate_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.validate_data.ENVIRONMENT", "test")
    @patch("src.etl.transform_data.S3_BUCKET", "test-bucket")
    @patch("src.etl.transform_data.ENVIRONMENT", "test")
    def test_data_statistics_consistent_across_all_stages(
        self,
        mock_transform_save,
        mock_transform_load,
        mock_validate_save_report,
        mock_validate_load,
        mock_fetch_save,
        mock_sleep,
        mock_read_html,
    ):
        """
        Test data consistency through fetch → validate → transform.
        Verify player counts remain consistent at each stage.
        """
        import pandas as pd

        player_count = 450

        # Mock complete Basketball-Reference data
        per_game_stats = [
            create_basketball_reference_player_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 5.0, 4.0
            )
            for i in range(player_count)
        ]
        mock_pergame_df = pd.DataFrame(per_game_stats)
        advanced_stats = [
            create_basketball_reference_advanced_stats(
                f"Player {i}", "PG", 25, "LAL", 60, 15.0, 2.0
            )
            for i in range(player_count)
        ]
        mock_advanced_df = pd.DataFrame(advanced_stats)
        mock_read_html.side_effect = [[mock_pergame_df], [mock_advanced_df]]

        # Simulate S3 storage
        s3_storage = {}

        def save_impl(data, key):
            s3_storage[key] = data
            return True

        def load_impl(key):
            return s3_storage.get(key)

        mock_fetch_save.side_effect = save_impl
        mock_validate_load.side_effect = load_impl
        mock_transform_load.side_effect = load_impl
        mock_transform_save.side_effect = save_impl

        # Stage 1: Fetch
        from src.etl import fetch_data

        fetch_result = fetch_data.handler(
            {"fetch_type": "stats_only", "season": "2025-26"}, MagicMock()
        )

        # Verify fetch stored correct player count
        stats_key = [k for k in s3_storage.keys() if "league_player_stats" in k][0]
        fetched_stats = s3_storage[stats_key]
        assert len(fetched_stats["per_game_stats"]) == player_count

        # Stage 2: Validate
        validate_result = validate_data.handler(
            {"data_location": fetch_result["data_location"], "fetch_type": "stats_only"},
            MagicMock(),
        )

        assert validate_result["validation_passed"] is True

        # Stage 3: Transform
        from src.etl import transform_data

        transform_result = transform_data.handler(
            {
                "validation_passed": validate_result["validation_passed"],
                "data_location": validate_result["data_location"],
            },
            MagicMock(),
        )

        # Verify transform maintained player count
        enriched_stats_key = [k for k in s3_storage.keys() if "enriched_player_stats" in k][0]
        enriched_stats = s3_storage[enriched_stats_key]
        assert len(enriched_stats["player_stats"]) == player_count
        assert enriched_stats["statistics"]["total_players"] == player_count

        # Verify consistency across all stages
        transform_body = json.loads(transform_result["body"])
        assert transform_body["statistics"]["total_player_stats"] == player_count
